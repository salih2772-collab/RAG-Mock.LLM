{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBd-pF1eWjBJ",
        "outputId": "9fa0903a-656e-4be7-d7b1-2c13a50a54cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn python-dotenv\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAYJcGkkaWbB",
        "outputId": "50a83c30-0715-418b-ed15-aa4f2d705293"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI(title=\"Modular RAG API\")\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "def health_check():\n",
        "    return {\"status\": \"ok\"}\n"
      ],
      "metadata": {
        "id": "DfQIB8g_aksJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI(title=\"Modular RAG API\")\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "def health_check():\n",
        "    return {\"status\": \"ok\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZNWj5vLcdhd",
        "outputId": "92a41864-337c-4d76-d73c-0e652f0b2fff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUnCsl67ck1t",
        "outputId": "3c9b2418-d71c-4ee2-8a39-499f254ea72a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py  __pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f uvicorn\n"
      ],
      "metadata": {
        "id": "vI8kDTPVfY7W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn main:app --host 0.0.0.0 --port 8000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p6l78dGRfkr3",
        "outputId": "a7fdd30b-e3b6-4531-da37-394bf37ae202"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m13735\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m13735\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 678, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 157, in _on_sigint\n",
            "    raise KeyboardInterrupt()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 701, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
            "    return await self.receive_queue.get()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/queues.py\", line 158, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\"http://127.0.0.1:8000/health\")\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2OfDkjflHl-",
        "outputId": "5095c5d1-057f-43fc-f0c2-36d5548f5746"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'status': 'ok'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0T-EIL6sWvq",
        "outputId": "a0891a76-3842-48ae-90c8-5b3623210ff3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm_factory.py\n",
        "from typing import Any\n",
        "\n",
        "class BaseLLM:\n",
        "    \"\"\"Abstract base class for all LLMs\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        raise NotImplementedError(\"LLM must implement generate method\")\n",
        "\n",
        "\n",
        "class GPTLLM(BaseLLM):\n",
        "    def __init__(self, api_key: str):\n",
        "        import openai\n",
        "        openai.api_key = api_key\n",
        "        self.client = openai\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        response = self.client.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "class DeepSeekLLM(BaseLLM):\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        return f\"[DeepSeek] simulated response for: {prompt}\"\n",
        "\n",
        "\n",
        "class GeminiLLM(BaseLLM):\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        return f\"[Gemini] simulated response for: {prompt}\"\n",
        "\n",
        "\n",
        "class LLMFactory:\n",
        "    @staticmethod\n",
        "    def create(model_provider: str, **kwargs) -> BaseLLM:\n",
        "        provider = model_provider.lower()\n",
        "\n",
        "        if provider == \"openai\":\n",
        "            return GPTLLM(api_key=kwargs.get(\"api_key\", \"\"))\n",
        "        elif provider == \"deepseek\":\n",
        "            return DeepSeekLLM()\n",
        "        elif provider == \"gemini\":\n",
        "            return GeminiLLM()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown LLM provider: {model_provider}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj3nFNM4skOL",
        "outputId": "ee34c965-2be4-4c8e-857e-31002d15f4d8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm_factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ogSFrm-smtU",
        "outputId": "4b64d3ab-9213-441d-cd3d-fd160bb5d54a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm_factory.py\tmain.py  __pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, Query\n",
        "from pydantic import BaseModel\n",
        "from llm_factory import LLMFactory\n",
        "\n",
        "app = FastAPI(title=\"Modular RAG API\")\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "def health_check():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "@app.get(\"/test-llm\")\n",
        "def test_llm(model: str = Query(...), prompt: str = Query(...)):\n",
        "    llm = LLMFactory.create(\n",
        "        model_provider=model,\n",
        "        api_key=\"DUMMY_KEY\"\n",
        "    )\n",
        "    response = llm.generate(prompt)\n",
        "    return {\"model\": model, \"response\": response}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apTGaBJ1tuB3",
        "outputId": "79ba980c-915e-4cf5-a517-15be7ddd34a3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f uvicorn"
      ],
      "metadata": {
        "id": "7qLX_1lZt4hp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\n",
        "    \"http://127.0.0.1:8000/test-llm\",\n",
        "    params={\"model\": \"deepseek\", \"prompt\": \"Hello RAG\"}\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eUajKwzvBjC",
        "outputId": "d8d78734-295b-45c7-c293-4802cf8eef84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'deepseek', 'response': '[DeepSeek] simulated response for: Hello RAG'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_loader.py\n",
        "... (کد loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VnIVfRZHX_2",
        "outputId": "ed485f53-868a-4ccf-ea8c-1757e3779c9c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSubz7eeFRVa",
        "outputId": "90e24afe-969e-4c54-b4f7-1e79fecebf4f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  data_loader.py  llm_factory.py  main.py  __pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data/sample.txt\n",
        "This is a sample text file.\n",
        "It contains internal company policy information.\n",
        "The termination clause is explained in section 4.\n",
        "Employees must follow the guidelines strictly.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74NAqRuxFh_E",
        "outputId": "06901fbc-0b42-46c9-c28f-34e06681969f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data/sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "pdf.multi_cell(\n",
        "    0,\n",
        "    8,\n",
        "    \"This PDF document describes a contract agreement.\\n\"\n",
        "    \"Termination is allowed with a 30 days written notice.\\n\"\n",
        "    \"This agreement is governed by internal regulations.\"\n",
        ")\n",
        "pdf.output(\"data/sample.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WnnaoR8XFlzE",
        "outputId": "af460520-bd4c-4df1-b879-03738c15a7ad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw-zRjh_G0t3",
        "outputId": "f073cf56-88d1-4943-8dd4-8fb83d53eea1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  data_loader.py  llm_factory.py  main.py  __pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOsSS8S2GFSJ",
        "outputId": "41da8392-2982-4106-89b5-e18dc1242e39"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample.pdf  sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_loader.py\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from pypdf import PdfReader\n",
        "\n",
        "\n",
        "def load_txt(file_path: Path) -> Dict:\n",
        "    text = file_path.read_text(encoding=\"utf-8\")\n",
        "    return {\n",
        "        \"content\": text,\n",
        "        \"source\": file_path.name,\n",
        "        \"type\": \"txt\"\n",
        "    }\n",
        "\n",
        "\n",
        "def load_pdf(file_path: Path) -> Dict:\n",
        "    reader = PdfReader(str(file_path))\n",
        "    pages = [page.extract_text() or \"\" for page in reader.pages]\n",
        "    text = \"\\n\".join(pages)\n",
        "    return {\n",
        "        \"content\": text,\n",
        "        \"source\": file_path.name,\n",
        "        \"type\": \"pdf\"\n",
        "    }\n",
        "\n",
        "\n",
        "def load_documents(data_dir: str) -> List[Dict]:\n",
        "    documents = []\n",
        "    path = Path(data_dir)\n",
        "\n",
        "    for file in path.iterdir():\n",
        "        if file.suffix.lower() == \".txt\":\n",
        "            documents.append(load_txt(file))\n",
        "        elif file.suffix.lower() == \".pdf\":\n",
        "            documents.append(load_pdf(file))\n",
        "\n",
        "    return documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCTF8zO_IsSF",
        "outputId": "84a4d800-1995-4498-8f68-ffa5f615a7e7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,20p' data_loader.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjxPHFOlKrwK",
        "outputId": "82e28700-a2db-4f46-a27b-76bdb17fe902"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from pathlib import Path\n",
            "from typing import List, Dict\n",
            "from pypdf import PdfReader\n",
            "\n",
            "\n",
            "def load_txt(file_path: Path) -> Dict:\n",
            "    text = file_path.read_text(encoding=\"utf-8\")\n",
            "    return {\n",
            "        \"content\": text,\n",
            "        \"source\": file_path.name,\n",
            "        \"type\": \"txt\"\n",
            "    }\n",
            "\n",
            "\n",
            "def load_pdf(file_path: Path) -> Dict:\n",
            "    reader = PdfReader(str(file_path))\n",
            "    pages = [page.extract_text() or \"\" for page in reader.pages]\n",
            "    text = \"\\n\".join(pages)\n",
            "    return {\n",
            "        \"content\": text,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_loader import load_documents\n",
        "\n",
        "documents = load_documents(\"data\")\n",
        "\n",
        "for doc in documents:\n",
        "    print(\"SOURCE:\", doc[\"source\"])\n",
        "    print(\"TYPE:\", doc[\"type\"])\n",
        "    print(\"CONTENT PREVIEW:\")\n",
        "    print(doc[\"content\"][:200])\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n93vNA6PK8HH",
        "outputId": "ce970476-ca0c-486c-ffea-9e8e474ff7f8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOURCE: sample.txt\n",
            "TYPE: txt\n",
            "CONTENT PREVIEW:\n",
            "This is a sample text file.\n",
            "It contains internal company policy information.\n",
            "The termination clause is explained in section 4.\n",
            "Employees must follow the guidelines strictly.\n",
            "\n",
            "--------------------------------------------------\n",
            "SOURCE: sample.pdf\n",
            "TYPE: pdf\n",
            "CONTENT PREVIEW:\n",
            "This PDF document describes a contract agreement.\n",
            "Termination is allowed with a 30 days written notice.\n",
            "This agreement is governed by internal regulations.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chunker.py\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def chunk_text(\n",
        "    text: str,\n",
        "    chunk_size: int = 500,\n",
        "    overlap: int = 100\n",
        ") -> List[str]:\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_length = len(text)\n",
        "\n",
        "    while start < text_length:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_documents(documents: List[Dict]) -> List[Dict]:\n",
        "    all_chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        chunks = chunk_text(doc[\"content\"])\n",
        "\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"content\": chunk,\n",
        "                \"source\": doc[\"source\"],\n",
        "                \"type\": doc[\"type\"],\n",
        "                \"chunk_id\": idx\n",
        "            })\n",
        "\n",
        "    return all_chunks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGULhcDlM-PJ",
        "outputId": "e401287a-a9fd-491b-b011-e0fa3413efb4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chunker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_loader import load_documents\n",
        "from chunker import chunk_documents\n",
        "\n",
        "documents = load_documents(\"data\")\n",
        "chunks = chunk_documents(documents)\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\\n\")\n",
        "\n",
        "for c in chunks[:5]:\n",
        "    print(\"SOURCE:\", c[\"source\"])\n",
        "    print(\"CHUNK ID:\", c[\"chunk_id\"])\n",
        "    print(\"CONTENT PREVIEW:\")\n",
        "    print(c[\"content\"][:200])\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll8JaWmsNkRq",
        "outputId": "bfb9fb5e-dab1-4d87-9660-69e2a653ded3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 2\n",
            "\n",
            "SOURCE: sample.txt\n",
            "CHUNK ID: 0\n",
            "CONTENT PREVIEW:\n",
            "This is a sample text file.\n",
            "It contains internal company policy information.\n",
            "The termination clause is explained in section 4.\n",
            "Employees must follow the guidelines strictly.\n",
            "\n",
            "--------------------------------------------------\n",
            "SOURCE: sample.pdf\n",
            "CHUNK ID: 0\n",
            "CONTENT PREVIEW:\n",
            "This PDF document describes a contract agreement.\n",
            "Termination is allowed with a 30 days written notice.\n",
            "This agreement is governed by internal regulations.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile embedding_factory.py\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class BaseEmbedding:\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "# ---- OpenAI Embedding (paid) ----\n",
        "class OpenAIEmbedding(BaseEmbedding):\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        from openai import OpenAI\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        response = self.client.embeddings.create(\n",
        "            model=self.model,\n",
        "            input=texts\n",
        "        )\n",
        "        return [item.embedding for item in response.data]\n",
        "\n",
        "\n",
        "# ---- Mock Embedding (free, dev/testing) ----\n",
        "class MockEmbedding(BaseEmbedding):\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        vectors = []\n",
        "        for text in texts:\n",
        "            # simple deterministic fake embedding\n",
        "            vectors.append([float(len(text))] * 10)\n",
        "        return vectors\n",
        "\n",
        "\n",
        "# ---- Factory ----\n",
        "class EmbeddingFactory:\n",
        "    @staticmethod\n",
        "    def create(provider: str, **kwargs) -> BaseEmbedding:\n",
        "        provider = provider.lower()\n",
        "\n",
        "        if provider == \"openai\":\n",
        "            return OpenAIEmbedding(\n",
        "                api_key=kwargs[\"api_key\"],\n",
        "                model=kwargs.get(\"model\", \"text-embedding-3-small\")\n",
        "            )\n",
        "\n",
        "        if provider == \"mock\":\n",
        "            return MockEmbedding()\n",
        "\n",
        "        raise ValueError(f\"Unknown embedding provider: {provider}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M2UFccrRmMJ",
        "outputId": "23e5c34a-f561-4036-e4b5-3b7594fcb471"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting embedding_factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile embedding_factory.py\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class BaseEmbedding:\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class MockEmbedding(BaseEmbedding):\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        vectors = []\n",
        "        for text in texts:\n",
        "            vectors.append([float(len(text))] * 10)\n",
        "        return vectors\n",
        "\n",
        "\n",
        "class EmbeddingFactory:\n",
        "    @staticmethod\n",
        "    def create(provider: str, **kwargs) -> BaseEmbedding:\n",
        "        provider = provider.lower()\n",
        "\n",
        "        if provider == \"mock\":\n",
        "            return MockEmbedding()\n",
        "\n",
        "        raise ValueError(f\"Unknown embedding provider: {provider}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BghkTe_1SBN7",
        "outputId": "3097d146-c6d7-4e55-d8f2-d5e7f82d2c93"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting embedding_factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile embedding_factory.py\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class BaseEmbedding:\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class MockEmbedding(BaseEmbedding):\n",
        "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [[float(len(text))] * 10 for text in texts]\n",
        "\n",
        "\n",
        "class EmbeddingFactory:\n",
        "    @staticmethod\n",
        "    def create(provider: str, **kwargs) -> BaseEmbedding:\n",
        "        provider = provider.lower()\n",
        "\n",
        "        if provider == \"mock\":\n",
        "            return MockEmbedding()\n",
        "\n",
        "        raise ValueError(f\"Unknown embedding provider: {provider}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYX-_uLkh9be",
        "outputId": "bce5dc06-dc85-4c39-e9a7-761015ed1a64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting embedding_factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from embedding_factory import EmbeddingFactory\n",
        "\n",
        "embedder = EmbeddingFactory.create(\"mock\")\n",
        "print(type(embedder))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlhSyFSTiJdG",
        "outputId": "af29983d-17fb-437e-ef9e-3c011bc254b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'embedding_factory.MockEmbedding'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_store.py\n",
        "from typing import List, Dict\n",
        "import math\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
        "    dot = sum(a * b for a, b in zip(vec1, vec2))\n",
        "    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
        "    norm2 = math.sqrt(sum(b * b for b in vec2))\n",
        "    return dot / (norm1 * norm2 + 1e-10)\n",
        "\n",
        "\n",
        "class InMemoryVectorStore:\n",
        "    def __init__(self):\n",
        "        self.vectors = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add(self, vectors: List[List[float]], metadatas: List[Dict]):\n",
        "        for v, m in zip(vectors, metadatas):\n",
        "            self.vectors.append(v)\n",
        "            self.metadata.append(m)\n",
        "\n",
        "    def search(self, query_vector: List[float], top_k: int = 3):\n",
        "        scores = []\n",
        "\n",
        "        for idx, vector in enumerate(self.vectors):\n",
        "            score = cosine_similarity(query_vector, vector)\n",
        "            scores.append((score, self.metadata[idx]))\n",
        "\n",
        "        scores.sort(key=lambda x: x[0], reverse=True)\n",
        "        return scores[:top_k]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3rQykNSX3rw",
        "outputId": "a167455c-3916-4743-c2f0-ee5be3a79e99"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vector_store.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_loader import load_documents\n",
        "from chunker import chunk_documents\n",
        "from embedding_factory import EmbeddingFactory\n",
        "from vector_store import InMemoryVectorStore\n",
        "\n",
        "documents = load_documents(\"data\")\n",
        "chunks = chunk_documents(documents)\n",
        "\n",
        "texts = [c[\"content\"] for c in chunks]\n",
        "metadata = [\n",
        "    {\n",
        "        \"source\": c[\"source\"],\n",
        "        \"chunk_id\": c[\"chunk_id\"],\n",
        "        \"preview\": c[\"content\"][:100]\n",
        "    }\n",
        "    for c in chunks\n",
        "]\n",
        "\n",
        "embedder = EmbeddingFactory.create(provider=\"mock\")\n",
        "vectors = embedder.embed(texts)\n",
        "\n",
        "store = InMemoryVectorStore()\n",
        "store.add(vectors, metadata)\n",
        "\n",
        "query = \"How can a contract be terminated?\"\n",
        "query_vector = embedder.embed([query])[0]\n",
        "\n",
        "results = store.search(query_vector, top_k=3)\n",
        "\n",
        "for score, meta in results:\n",
        "    print(\"Score:\", score)\n",
        "    print(\"Source:\", meta[\"source\"])\n",
        "    print(\"Chunk ID:\", meta[\"chunk_id\"])\n",
        "    print(\"Preview:\", meta[\"preview\"])\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUw5VUjcYC3D",
        "outputId": "c6077fbe-01bb-473c-9e47-d0977794b875"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.9999999999999982\n",
            "Source: sample.txt\n",
            "Chunk ID: 0\n",
            "Preview: This is a sample text file.\n",
            "It contains internal company policy information.\n",
            "The termination clause \n",
            "----------------------------------------\n",
            "Score: 0.999999999999998\n",
            "Source: sample.pdf\n",
            "Chunk ID: 0\n",
            "Preview: This PDF document describes a contract agreement.\n",
            "Termination is allowed with a 30 days written noti\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm_factory.py\n",
        "class BaseLLM:\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class MockLLM(BaseLLM):\n",
        "    def generate(self, question: str, context: str) -> str:\n",
        "        return (\n",
        "            \"ANSWER (mock)\\n\\n\"\n",
        "            \"QUESTION:\\n\"\n",
        "            f\"{question}\\n\\n\"\n",
        "            \"CONTEXT USED:\\n\"\n",
        "            f\"{context}\\n\\n\"\n",
        "            \"NOTE: This answer is generated by a mock LLM.\"\n",
        "        )\n",
        "\n",
        "\n",
        "class LLMFactory:\n",
        "    @staticmethod\n",
        "    def create(provider: str) -> BaseLLM:\n",
        "        provider = provider.lower()\n",
        "\n",
        "        if provider == \"mock\":\n",
        "            return MockLLM()\n",
        "\n",
        "        raise ValueError(f\"Unknown LLM provider: {provider}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLk-DD_5jMd2",
        "outputId": "20acaafc-7fef-4afa-9412-7ad8f3005add"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting llm_factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_pipeline.py\n",
        "from embedding_factory import EmbeddingFactory\n",
        "from vector_store import InMemoryVectorStore\n",
        "from llm_factory import LLMFactory\n",
        "\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, store: InMemoryVectorStore):\n",
        "        self.store = store\n",
        "        self.embedder = EmbeddingFactory.create(provider=\"mock\")\n",
        "        self.llm = LLMFactory.create(provider=\"mock\")\n",
        "\n",
        "    def answer(self, question: str, top_k: int = 3) -> str:\n",
        "        query_vector = self.embedder.embed([question])[0]\n",
        "        results = self.store.search(query_vector, top_k=top_k)\n",
        "\n",
        "        context_parts = []\n",
        "        for score, meta in results:\n",
        "            context_parts.append(meta[\"preview\"])\n",
        "\n",
        "        context = \"\\n---\\n\".join(context_parts)\n",
        "\n",
        "        return self.llm.generate(question=question, context=context)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfD0jRTPjTrD",
        "outputId": "03239319-a632-4e48-c1e5-70829f0194a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing rag_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from data_loader import load_documents\n",
        "from chunker import chunk_documents\n",
        "from embedding_factory import EmbeddingFactory\n",
        "from vector_store import InMemoryVectorStore\n",
        "from rag_pipeline import RAGPipeline\n",
        "\n",
        "\n",
        "app = FastAPI(title=\"RAG API (Mock)\")\n",
        "\n",
        "# ---- Build knowledge base at startup ----\n",
        "documents = load_documents(\"data\")\n",
        "chunks = chunk_documents(documents)\n",
        "\n",
        "texts = [c[\"content\"] for c in chunks]\n",
        "metadata = [\n",
        "    {\n",
        "        \"source\": c[\"source\"],\n",
        "        \"chunk_id\": c[\"chunk_id\"],\n",
        "        \"preview\": c[\"content\"][:200]\n",
        "    }\n",
        "    for c in chunks\n",
        "]\n",
        "\n",
        "embedder = EmbeddingFactory.create(provider=\"mock\")\n",
        "vectors = embedder.embed(texts)\n",
        "\n",
        "store = InMemoryVectorStore()\n",
        "store.add(vectors, metadata)\n",
        "\n",
        "rag = RAGPipeline(store)\n",
        "\n",
        "\n",
        "# ---- API schema ----\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "\n",
        "class AnswerResponse(BaseModel):\n",
        "    answer: str\n",
        "\n",
        "\n",
        "@app.post(\"/ask\", response_model=AnswerResponse)\n",
        "def ask_question(req: QuestionRequest):\n",
        "    answer = rag.answer(req.question)\n",
        "    return {\"answer\": answer}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz1zSArtjZO1",
        "outputId": "94699629-23f3-4c33-d16a-5330cc22de76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://127.0.0.1:8000/ask\",\n",
        "    json={\"question\": \"How can a contract be terminated?\"}\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "418lCcNjkAIa",
        "outputId": "6f76d9f9-ba37-405a-ac2d-daf6be7bc1ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'ANSWER (mock)\\n\\nQUESTION:\\nHow can a contract be terminated?\\n\\nCONTEXT USED:\\nThis is a sample text file.\\nIt contains internal company policy information.\\nThe termination clause is explained in section 4.\\nEmployees must follow the guidelines strictly.\\n\\n---\\nThis PDF document describes a contract agreement.\\nTermination is allowed with a 30 days written notice.\\nThis agreement is governed by internal regulations.\\n\\nNOTE: This answer is generated by a mock LLM.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X63Y0rdasrI_",
        "outputId": "28b44ba3-d797-4a76-e2e5-bcb36e970a24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunker.py  data_loader.py        llm_factory.py  rag_pipeline.py\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/       embedding_factory.py  main.py         vector_store.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "fastapi\n",
        "uvicorn\n",
        "pydantic\n",
        "pypdf\n",
        "fpdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCvQcQlntMuy",
        "outputId": "04e485aa-736c-4762-fc30-54800eff9ac3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "# RAG From Scratch (Vendor-Agnostic)\n",
        "\n",
        "This project implements a complete Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
        "\n",
        "## Features\n",
        "- PDF and TXT ingestion\n",
        "- Deterministic chunking with overlap\n",
        "- Pluggable embedding providers (mock)\n",
        "- In-memory vector store with cosine similarity\n",
        "- Retrieval-based context injection\n",
        "- Pluggable LLM providers (mock)\n",
        "- FastAPI backend\n",
        "\n",
        "## Architecture\n",
        "Documents\n",
        "→ Chunking\n",
        "→ Embeddings\n",
        "→ Vector Store\n",
        "→ Retrieval\n",
        "→ LLM\n",
        "\n",
        "## Run locally\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "uvicorn main:app --reload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8CDcp19tR0O",
        "outputId": "3bd080b0-ca51-480c-fdc3-8a00d4cb2f36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://127.0.0.1:8000/ask\",\n",
        "    json={\n",
        "        \"question\": \"How can a contract be terminated?\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.json())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBR40n--tb05",
        "outputId": "8de4ba02-7028-4e46-fdac-4102f77becad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'ANSWER (mock)\\n\\nQUESTION:\\nHow can a contract be terminated?\\n\\nCONTEXT USED:\\nThis is a sample text file.\\nIt contains internal company policy information.\\nThe termination clause is explained in section 4.\\nEmployees must follow the guidelines strictly.\\n\\n---\\nThis PDF document describes a contract agreement.\\nTermination is allowed with a 30 days written notice.\\nThis agreement is governed by internal regulations.\\n\\nNOTE: This answer is generated by a mock LLM.'}\n"
          ]
        }
      ]
    }
  ]
}